# yolo_stream.py

import cv2
from ultralytics import YOLO
import numpy as np
import time

# Load the YOLO11 model (ëª¨ë¸ì€ í•œ ë²ˆë§Œ ë¡œë“œ)
# ì „ì—­ ë³€ìˆ˜ë¡œ ì„ ì–¸í•˜ì—¬ ë£¨í”„ë§ˆë‹¤ ë¡œë“œí•˜ì§€ ì•Šë„ë¡ í•©ë‹ˆë‹¤.
model = YOLO("yolo11n.pt")
# ì¶”ì  ê¸°ëŠ¥ì„ ìœ„í•´ persist=Trueë¥¼ ìœ ì§€
tracker_args = dict(
    classes=[0],
    conf=0.5,
    max_det=10,
    tracker="bytetrack.yaml",
    persist=True
)


def video_processing_generator(video_path):
    """
    ë¹„ë””ì˜¤ íŒŒì¼ì„ ì²˜ë¦¬í•˜ê³  ì‹œê°í™”ëœ JPEG í”„ë ˆì„ì„ yieldí•˜ëŠ” ì œë„ˆë ˆì´í„°
    """
    cap = cv2.VideoCapture(video_path)
    prev_time = 0 
    
    print("\n=======================================================")
    print("           YOLO Tracking Console Log Started")
    print("=======================================================")

    while cap.isOpened():
        success, frame = cap.read()

        if not success:
            break

        # FPS ê³„ì‚°
        current_time = time.time()
        fps = 1 / (current_time - prev_time) if prev_time != 0 else 0
        prev_time = current_time 
        fps_text = f"FPS: {fps:.2f}" 

        # YOLO ì¶”ì  ì‹¤í–‰
        results = model.track(frame, **tracker_args)
        
        # 2. Visualize (ë°”ìš´ë”© ë°•ìŠ¤)
        annotated_frame = results[0].plot()

        # 3. ê°ì²´ ì¤‘ì‹¬ì  ë° ID ì¶”ì¶œ/ì˜¤ë²„ë ˆì´ (ì˜ìƒ ì¶œë ¥ ë¡œì§)
        boxes = results[0].boxes
        box_data = boxes.xyxy.cpu().numpy()
        
        # ì¶”ì  ID ë°ì´í„°
        if boxes.id is not None:
            track_ids = boxes.id.cpu().numpy().astype(int)
        else:
            track_ids = []

        
        # --------------------------------------------------------
        # ğŸ”¥ ì½˜ì†” ì¶œë ¥ ë¡œì§ ì¶”ê°€
        # --------------------------------------------------------
        print(f"\n[{fps_text}] - Detected Objects: {len(box_data)}")
        
        if len(box_data) > 0:
            for i, box in enumerate(box_data):
                x1, y1, x2, y2 = map(int, box[:4])
                center_x = (x1 + x2) // 2
                center_y = (y1 + y2) // 2
                
                current_id = track_ids[i] if len(track_ids) > i else "N/A"
                
                # ê°ì²´ ì •ë³´ ì½˜ì†” ì¶œë ¥
                print(f"  > ID: {current_id:<3} | Center X, Y: ({center_x:<4}, {center_y:<4}) | BBox: ({x1}, {y1}) to ({x2}, {y2})")

                # --- ì˜ìƒ ì˜¤ë²„ë ˆì´ (ê¸°ì¡´ ë¡œì§ ìœ ì§€) ---
                cv2.circle(annotated_frame, (center_x, center_y), 5, (0, 0, 255), -1)
                coord_text = f"({center_x}, {center_y})"
                cv2.putText(annotated_frame, coord_text, (center_x + 10, center_y + 5), 
                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)
        
        # --------------------------------------------------------
        
        # 4. FPS í…ìŠ¤íŠ¸ (ë¹¨ê°„ìƒ‰)
        cv2.putText(annotated_frame, fps_text, (10, 30), 
                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)

        # 5. í”„ë ˆì„ì„ JPEGë¡œ ì¸ì½”ë”©í•˜ì—¬ ì „ì†¡ ì¤€ë¹„
        (flag, encoded_image) = cv2.imencode(".jpg", annotated_frame, [cv2.IMWRITE_JPEG_QUALITY, 50])
        
        if flag:
            yield encoded_image.tobytes() 

        prev_time = current_time
    
    cap.release()
    print("=======================================================")
    print("               Video Stream Processing Ended")
    print("=======================================================")